import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Deep learning and embeddings
import gensim
from gensim.models import Word2Vec
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow as tf

## 1. Load and Prepare Data
def load_data():
    real = pd.read_csv("True.csv")
    fake = pd.read_csv("Fake.csv")
    
    real['label'] = 0
    fake['label'] = 1
    
    data = pd.concat([real, fake]).sample(frac=1).reset_index(drop=True)
    return data['text'], data['label']

X, y = load_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
## 2. Traditional ML Models with TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)
def train_eval_model(model, name):
    model.fit(X_train_tfidf, y_train)
    preds = model.predict(X_test_tfidf)
    acc = accuracy_score(y_test, preds)
    print(f"\n{name} Results:")
    print(f"Accuracy: {acc:.2%}")
    print(classification_report(y_test, preds))
    return acc
# Test Random Forest and SVM
rf_acc = train_eval_model(RandomForestClassifier(n_estimators=100), "Random Forest")
svm_acc = train_eval_model(SVC(kernel='linear', probability=True), "SVM")

## 3. Word Embeddings Approach (Word2Vec)
# Tokenize text
sentences = [text.split() for text in X_train]

# Train Word2Vec model
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
# Create document vectors by averaging word vectors
def document_vector(text):
    words = text.split()
    words = [word for word in words if word in w2v_model.wv]
    if len(words) == 0:
        return np.zeros(100)
    return np.mean(w2v_model.wv[words], axis=0)
X_train_w2v = np.array([document_vector(text) for text in X_train])
X_test_w2v = np.array([document_vector(text) for text in X_test])
# Train classifier on Word2Vec features
w2v_acc = train_eval_model(RandomForestClassifier(n_estimators=100), "Word2Vec + Random Forest")
## 4. LSTM Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
# Tokenize text
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)
# Pad sequences
max_len = 200
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)

# Build LSTM model
lstm_model = Sequential([
    Embedding(10000, 128, input_length=max_len),
    LSTM(64, dropout=0.2),
    Dense(1, activation='sigmoid')
])
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = lstm_model.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.1)
# Evaluate LSTM
lstm_preds = (lstm_model.predict(X_test_pad) > 0.5).astype("int32")
lstm_acc = accuracy_score(y_test, lstm_preds)
print("\nLSTM Results:")
print(f"Accuracy: {lstm_acc:.2%}")
print(classification_report(y_test, lstm_preds))
## 5. BERT Model (Simplified)
# Note: This requires significant RAM and GPU resources
try:
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')
    
    # Tokenize data (simplified - in practice need batching)
    train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)
    test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=128)
    
    # Convert to TensorFlow datasets
    train_dataset = tf.data.Dataset.from_tensor_slices((
        dict(train_encodings),
        y_train
    ))
    # Compile and train (simplified)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(train_dataset.shuffle(1000).batch(16), epochs=2)
    # Evaluate
    test_dataset = tf.data.Dataset.from_tensor_slices((
        dict(test_encodings),
        y_test
    ))
    bert_results = model.evaluate(test_dataset.batch(16))
    bert_acc = bert_results[1]
    print(f"\nBERT Accuracy: {bert_acc:.2%}")
except Exception as e:
    print(f"\nCould not run BERT (requires GPU): {e}")
    bert_acc = 0

## 6. Compare All Models
models = ['Random Forest', 'SVM', 'Word2Vec+RF', 'LSTM', 'BERT']
accuracies = [rf_acc, svm_acc, w2v_acc, lstm_acc, bert_acc]

plt.figure(figsize=(10, 5))
plt.bar(models, accuracies)
plt.title('Model Comparison')
plt.ylabel('Accuracy')
plt.ylim(0.8, 1.0)
plt.savefig('model_comparison.png')
plt.show()
